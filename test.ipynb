{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchors = [[5,5,5],[10,10,10],[15,15,15],[20,20,20],[30,30,30],[5,5,2.5],[10,10,5],[15,15,7.5],[20,20,10],[30,30,15]]\n",
    "# anchors = [[5,5,5],[10,10,10],[15,15,15],[20,20,20],[30,30,30]]\n",
    "# anchors = [[6,6,6],[12,12,12],[24,24,24],[9,9,6],[18,18,12],[36,36,24]]\n",
    "anchors = [[5,5,3],[8,8,4.8],[14,14,8.4],[28,28,16.8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_rpn_windows(f_shape):\n",
    "    \"\"\"\n",
    "    Generating anchor boxes at each voxel on the feature map,\n",
    "    the center of the anchor box on each voxel corresponds to center\n",
    "    on the original input image.\n",
    "\n",
    "    return\n",
    "    windows: list of anchor boxes, [z, y, x, d, h, w]\n",
    "    \"\"\"\n",
    "    stride = 4\n",
    "    anchors = np.asarray(anchors)\n",
    "    offset = (float(stride) - 1) / 2\n",
    "    _, _, D, H, W = f_shape\n",
    "    oz = np.arange(offset, offset + stride * (D - 1) + 1, stride)\n",
    "    oh = np.arange(offset, offset + stride * (H - 1) + 1, stride)\n",
    "    ow = np.arange(offset, offset + stride * (W - 1) + 1, stride)\n",
    "\n",
    "    windows = []\n",
    "    for z, y, x, a in itertools.product(oz, oh , ow , anchors):\n",
    "        windows.append([z, y, x, a[0], a[1], a[2]])\n",
    "    windows = np.array(windows)\n",
    "\n",
    "    return windows\n",
    "\n",
    "def find_nearest(x,y,z):\n",
    "    stride = 4\n",
    "    start = (stride-1) / 2\n",
    "    z_stride = 4\n",
    "    z_start = (z_stride-1) / 2\n",
    "    xl = ((x - start) // stride) *stride + start\n",
    "    xr = xl+stride\n",
    "    xn = xl if x-xl < xr-x else xr\n",
    "\n",
    "    yl = ((y - start) // stride) *stride + start\n",
    "    yr = yl+stride\n",
    "    yn = yl if y-yl < yr-y else yr\n",
    "\n",
    "    zl = ((z - z_start) // z_stride) *z_stride + z_start\n",
    "    zr = zl+z_stride\n",
    "    zn = zl if z-zl < zr-z else zr\n",
    "\n",
    "    return xn, yn, zn\n",
    "\n",
    "def IoU(bbox1, bbox2):\n",
    "    intersection_min = np.maximum(bbox1[:3], bbox2[:3])\n",
    "    intersection_max = np.minimum(bbox1[3:], bbox2[3:])\n",
    "    intersection_dims = np.maximum(0, intersection_max - intersection_min)\n",
    "\n",
    "    intersection_volume = np.prod(intersection_dims)\n",
    "\n",
    "    volume1 = np.prod(bbox1[3:] - bbox1[:3])\n",
    "    volume2 = np.prod(bbox2[3:] - bbox2[:3])\n",
    "    union_volume = volume1 + volume2 - intersection_volume\n",
    "\n",
    "    iou = intersection_volume / union_volume if union_volume > 0 else 0\n",
    "    return iou\n",
    "\n",
    "\n",
    "def distance(cord1, cord2):\n",
    "    return ((cord1 - cord2)**2).sum()\n",
    "\n",
    "def DIoU(cord1, shape1, cord2, shape2):\n",
    "    box1 = np.array([*cord1-shape1/2, *cord1+shape1/2])\n",
    "    box2 = np.array([*cord2-shape2/2, *cord2+shape2/2])\n",
    "\n",
    "    c1 = np.amax([box1,box2],axis=0)[3:]\n",
    "    c2 = np.amin([box1,box2],axis=0)[:3]\n",
    "\n",
    "    return IoU(box1, box2) - distance(cord1, cord2)/distance(c1, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29.282228000000003,\n",
       " 17.7723395,\n",
       " 47.3686525,\n",
       " 42.184572,\n",
       " 27.097534500000002,\n",
       " 55.3666995]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cord = np.array([35.7334   , 22.434937 , 51.367676])\n",
    "shape = np.array([12.902344 ,  9.325195 ,         7.998047 ])\n",
    "[*cord-shape/2, *cord+shape/2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06666666666666667\n",
      "-0.04444444444444444\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "target=[[ 0      ,  0.       ,  0.       ,   4,  4.       ,         4 ],\n",
    "        [ 88.5      ,  66.       ,  96.       ,  10.       ,  13.       ,         12.999999 ],\n",
    "        [ 88.5      ,  66.       ,  96.       ,  10.       ,  13.       ,         12.999999 ],\n",
    "        [ 88.5      ,  66.       ,  96.       ,  10.       ,  12.999999 ,         12.999999 ]]\n",
    "pred = [[ 2      ,  2.       ,  2.       ,   4,  4.       ,         4 ],\n",
    "        [ 88.5      ,  66.       ,  96.       ,  10.       ,  13.       ,         12.999999 ],\n",
    "        [ 88.5      ,  66.       ,  96.       ,  10.       ,  13.       ,         12.999999 ],\n",
    "        [ 88.5      ,  66.       ,  96.       ,  10.       ,  12.999999 ,         12.999999 ]]\n",
    "target = np.array(target)\n",
    "pred = np.array(pred)\n",
    "\n",
    "# DIoU(p[:,:3], p[:,3:], t[:,:3], t[:,3:])\n",
    "for p ,t in zip(pred, target):\n",
    "    print(DIoU(p[:3], p[3:],t[:3], t[3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 845, 1475, 84] \n",
      " [0, 3, 46, 1761] \n",
      " [1, 1782, 27]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "nodule_distribution = [0,0,0,0]\n",
    "anchor_distribution = [0 for i in range(len(anchors))]\n",
    "\n",
    "train_txt = r'F:\\master\\code\\Lung_Nodule\\FL_lung_nodule_datasplit_ME_LDCT\\client0_train.txt'\n",
    "with open(train_txt, 'r') as f:\n",
    "    train_files = f.readlines()\n",
    "anchor_count = [0,0,0]\n",
    "bad_nodule = []\n",
    "for train_file in train_files[1:]:\n",
    "    p1, p2 = train_file.replace('\\n','').split(',')\n",
    "\n",
    "    dicom_nodule_path = os.path.join(p1, 'mask', f'{p2}_nodule_count.json') # CHEST1001_nodule_count\n",
    "    with open(dicom_nodule_path, 'r') as f:\n",
    "        dicom_nodule = json.load(f)\n",
    "    nodules = dicom_nodule['bboxes']\n",
    "    for nodule in nodules:\n",
    "        good_nodule = 0\n",
    "        cx = (nodule[0][0]+nodule[1][0])/2\n",
    "        cy = (nodule[0][1]+nodule[1][1])/2\n",
    "        cz = (nodule[0][2]+nodule[1][2])/2\n",
    "        w = (nodule[1][0]-nodule[0][0])+4\n",
    "        h = (nodule[1][1]-nodule[0][1])+4\n",
    "        d = (nodule[1][2]-nodule[0][2])+4\n",
    "\n",
    "        size = (4/3)*3.1415*(h*w*d / 6)\n",
    "        if size <= 52:\n",
    "            size_idx = 0\n",
    "        elif size <= 113:\n",
    "            size_idx = 1\n",
    "        elif size <= 268:\n",
    "            size_idx = 2\n",
    "        else:\n",
    "            size_idx = 3\n",
    "\n",
    "        nodule_distribution[size_idx] += 1\n",
    "\n",
    "        nearest_point = find_nearest(cx, cy, cz)\n",
    "\n",
    "        for i, anchor in enumerate(anchors):\n",
    "            iou = IoU([cx,cy,cz], [w,h,d], nearest_point, anchor)\n",
    "            if iou >= 0.3:\n",
    "                anchor_distribution[i] += 1\n",
    "                good_nodule = 1\n",
    "                continue\n",
    "            elif iou >= 0.1 and good_nodule < 1:\n",
    "                good_nodule = 2\n",
    "\n",
    "        anchor_count[good_nodule] += 1\n",
    "        if not good_nodule:\n",
    "            bad_nodule.append([size_idx, w,h,d])\n",
    "print(anchor_distribution, '\\n', nodule_distribution, '\\n', anchor_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 7, 8, 7]]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_nodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08982469602420033"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '318.56763\t334.10632\t108.51063\t8.093452\t8.115494\t5.353752'\n",
    "b = '318.83087\t334.1272\t102.111305\t8.714816\t7.911281\t6.0994024'\n",
    "a = a.split('\\t')\n",
    "a_cord = [float(a_) for a_ in a[:3]]\n",
    "a_shape = [float(a_)+2. for a_ in a[3:]]\n",
    "b = b.split('\\t')\n",
    "b_cord = [float(b_) for b_ in b[:3]]\n",
    "b_shape = [float(b_)+2. for b_ in b[3:]]\n",
    "IoU(a_cord, a_shape, b_cord, b_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([9.231955, 9.961705, 7.010294], [10.86131, 10.294744, 7.4044294])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_shape, b_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_path = r'F:\\master\\code\\LSSANet-main\\MsaNet_R_results\\ME_LDCT\\AdamW0.0003_Bs6x4_OHEM10_bb0\\res\\72\\FROC\\submission_rpn.csv'\n",
    "gt_path  = r'F:\\master\\code\\Lung_Nodule\\FL_lung_nodule_datasplit_ME_LDCT\\client0_test_annotation.csv'\n",
    "\n",
    "import csv\n",
    "sub_by_file = {}\n",
    "with open(sub_path, newline='') as fs:\n",
    "    subs = csv.reader(fs,)\n",
    "    for sub in subs:\n",
    "        if sub[0] in sub_by_file.keys():\n",
    "            sub_by_file[sub[0]].append([float(xyzhwd) for xyzhwd in sub[1:8]])\n",
    "        elif sub[0] != 'series_id':\n",
    "            sub_by_file[sub[0]] = [[float(xyzhwd) for xyzhwd in sub[1:8]]]\n",
    "\n",
    "gt_by_file  = {}\n",
    "with open(gt_path, newline='') as fg:\n",
    "    gts = csv.reader(fg)\n",
    "    for gt in gts:\n",
    "        if gt[0] in gt_by_file.keys():\n",
    "            gt_by_file[gt[0]].append([float(xyzhwd) for xyzhwd in gt[1:8]])\n",
    "        elif gt[0] != 'series_id':\n",
    "            gt_by_file[gt[0]] = [[float(xyzhwd) for xyzhwd in gt[1:8]]]\n",
    "\n",
    "files = list(gt_by_file.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = []\n",
    "FN = []\n",
    "gt_IoU = []\n",
    "gt_DIoU = []\n",
    "for file in files:\n",
    "    gts = gt_by_file[file]\n",
    "    subs = sub_by_file[file]\n",
    "    for gt in gts:\n",
    "        is_cand = 0\n",
    "        re_detected = 0\n",
    "        max_iou = 0.0\n",
    "        max_diou = 0.0\n",
    "        gt_cord, gt_shape = gt[:3], gt[3:6]\n",
    "        for sub in subs:\n",
    "            iou = IoU(gt_cord, gt_shape, sub[:3], sub[3:6])\n",
    "            if iou >= 0.1:\n",
    "                if is_cand:\n",
    "                    re_detected += 1\n",
    "                is_cand = 1\n",
    "            max_iou = max(max_iou, iou)\n",
    "            max_diou = max(max_diou, DIoU(gt_cord, gt_shape, sub[:3], sub[3:6]))\n",
    "        gt_IoU.append([max_iou, max_diou])\n",
    "        gt_DIoU.append(max_diou)\n",
    "        if is_cand:\n",
    "            TP.append(max_iou)\n",
    "        else:\n",
    "            FN.append(max_iou)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "from single_config import config\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def crop_with_lobe(z:str, yx:str):\n",
    "    zlobe = z.replace('\\n','').split(',')\n",
    "    Ds, De = [int(z_) for z_ in zlobe]\n",
    "    yxlobe = yx.replace('\\n','').split(',')\n",
    "    Hs, He, Ws, We = [int(yx_) for yx_ in yxlobe]\n",
    "    \n",
    "    # align to 16\n",
    "    align = 16\n",
    "    Ds = (Ds//align)*align\n",
    "    De = ((De+align-1)//align)*align\n",
    "    Hs = (Hs//align)*align\n",
    "    He = ((He+align-1)//align)*align\n",
    "    Ws = (Ws//align)*align\n",
    "    We = ((We+align-1)//align)*align\n",
    "\n",
    "    return (Ds, De, Hs, He, Ws, We)\n",
    "\n",
    "def load_img(filename):\n",
    "    path, dir = filename.split(',')\n",
    "    img = np.load('%s\\\\npy\\\\%s.npy' % (path, dir))\n",
    "    # img = img[np.newaxis,...] # (y, x, z) -> (1, y, x, z)\n",
    "    ## load lobe info\n",
    "    with open('%s\\\\npy\\\\lobe_info.txt' %(path)) as f:\n",
    "        lobe_info = f.readlines()[-2:]\n",
    "    with open('%s\\\\mask\\\\%s_nodule_count.json' % (path, dir)) as f:\n",
    "        nodule_count = json.load(f)\n",
    "    bboxes = nodule_count['bboxes']\n",
    "    Ds, De, Hs, He, Ws, We = crop_with_lobe(*lobe_info)\n",
    "    # crop the lobe\n",
    "    img = img[np.newaxis, Hs:He, Ws:We, Ds:De]\n",
    "\n",
    "    img = np.clip(img, -1000, 400)\n",
    "    img = img.astype(np.float32)\n",
    "    img = img.transpose(0, 3, 1, 2) # (1, y, x, z) -> (1, z, y, x)\n",
    "    images = img + 1000    # 0 ~ max\n",
    "    images = (images - 700) / 700 # -1 ~ 1\n",
    "    return images, (Ds, De, Hs, He, Ws, We), bboxes\n",
    "\n",
    "class Crop(object):\n",
    "    def __init__(self, config):\n",
    "        self.crop_size = config['crop_size']\n",
    "        self.bound_size = config['bound_size']\n",
    "        self.stride = config['stride']\n",
    "        self.pad_value = config['pad_value']\n",
    "\n",
    "    def __call__(self, imgs, target, bboxes, lobe=None, isScale=False, isRand=False):\n",
    "        '''\n",
    "        img: 3D image loading from npy, (1, d, h, w)\n",
    "        target: one nodule\n",
    "        bboxes: all nodules in series\n",
    "        '''\n",
    "        if isScale:\n",
    "            radiusLim = [8.,120.]\n",
    "            scaleLim = [0.75,1.25]\n",
    "            scaleRange = [np.min([np.max([(radiusLim[0]/target[3]),scaleLim[0]]),1])\n",
    "                         ,np.max([np.min([(radiusLim[1]/target[3]),scaleLim[1]]),1])]\n",
    "            scale = np.random.rand()*(scaleRange[1]-scaleRange[0])+scaleRange[0]\n",
    "            crop_size = (np.array(self.crop_size).astype('float')/scale).astype('int')\n",
    "        else:\n",
    "            crop_size = self.crop_size\n",
    "        bound_size = self.bound_size\n",
    "        target = np.copy(target)\n",
    "        bboxes = np.copy(bboxes)\n",
    "        start = []\n",
    "        for i in range(3):\n",
    "            # start.append(int(target[i] - crop_size[i] / 2))\n",
    "            if not isRand:\n",
    "                # crop the sample base on target\n",
    "                r = target[i+3]/2\n",
    "                s = np.floor(target[i] - r) + 1 - bound_size\n",
    "                e = np.ceil(target[i] + r) + 1 + bound_size - crop_size[i]\n",
    "            else:\n",
    "                # crop the sample randomly\n",
    "                s = np.max([imgs.shape[i+1]-crop_size[i]/2, imgs.shape[i+1]/2+bound_size])\n",
    "                e = np.min([crop_size[i]/2, imgs.shape[i+1]/2-bound_size])\n",
    "                target = np.array([np.nan, np.nan, np.nan, np.nan])\n",
    "            if s > e:\n",
    "                i_start = np.random.randint(e, s)\n",
    "                i_start = max(min(i_start, imgs.shape[i+1]-crop_size[i]),0)\n",
    "                start.append(i_start)#!\n",
    "            else:\n",
    "                start.append(int(target[i])-crop_size[i]/2+np.random.randint(-bound_size/2,bound_size/2))\n",
    "\n",
    "        coord=[]\n",
    "        pad = []\n",
    "        pad.append([0,0])\n",
    "\n",
    "        for i in range(3):\n",
    "            leftpad = max(0,-start[i]) # how many pixel need to pad on the left side\n",
    "            rightpad = max(0,start[i]+crop_size[i]-imgs.shape[i+1]) # how many pixel need to pad on the right side\n",
    "            pad.append([leftpad,rightpad])\n",
    "            \n",
    "        crop = imgs[:,\n",
    "            int(max(start[0],0)):int(min(start[0] + int(crop_size[0]),imgs.shape[1])),\n",
    "            int(max(start[1],0)):int(min(start[1] + int(crop_size[1]),imgs.shape[2])),\n",
    "            int(max(start[2],0)):int(min(start[2] + int(crop_size[2]),imgs.shape[3]))]\n",
    "\n",
    "        crop = np.pad(crop, pad, 'constant', constant_values=0.67142)\n",
    "        for i in range(3):\n",
    "            target[i] = target[i] - start[i]\n",
    "        for i in range(len(bboxes)):\n",
    "            for j in range(3):\n",
    "                bboxes[i][j] = bboxes[i][j] - start[j]\n",
    "\n",
    "        if isScale:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                crop = zoom(crop, [1, scale, scale, scale], order=1)\n",
    "            newpad = self.crop_size[0] - crop.shape[1:][0]\n",
    "            if newpad < 0:\n",
    "                crop = crop[:, :-newpad, :-newpad, :-newpad]\n",
    "            elif newpad > 0:\n",
    "                pad2 = [[0, 0], [0, newpad], [0, newpad], [0, newpad]]\n",
    "                crop = np.pad(crop, pad2, 'constant', constant_values=0.67142)\n",
    "            for i in range(6):\n",
    "                target[i] = target[i] * scale\n",
    "            for i in range(len(bboxes)):\n",
    "                for j in range(6):\n",
    "                    bboxes[i][j] = bboxes[i][j] * scale\n",
    "\n",
    "        return crop, target, bboxes, coord\n",
    "\n",
    "crop = Crop(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = r'F:\\master\\code\\Lung_Nodule\\FL_lung_nodule_datasplit_ME_LDCT\\client0_test.txt'\n",
    "with open(test_txt, 'r') as f:\n",
    "    files = f.readlines()\n",
    "imgs, lobe_info, bboxes = load_img(files[1])\n",
    "bboxes = bboxes - [lobe_info[0], lobe_info[2], lobe_info[4], 0, 0, 0]\n",
    "for bbox in bboxes:\n",
    "    sample, target, bboxes, coord = crop(imgs, bbox, bboxes, isScale=False, isRand=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from utils.util import crop_with_lobe\n",
    "test_dicom_npy = r'F:\\master\\code\\Lung_Nodule\\dataset\\LDCT_test_dataset\\CHESTCT_Test0014\\npy\\CHESTCT_Test0014.npy'\n",
    "test_dicom_info = r'F:\\master\\code\\Lung_Nodule\\dataset\\LDCT_test_dataset\\CHESTCT_Test0014\\mask\\CHESTCT_Test0014_nodule_count.json'\n",
    "test_dicom_lobe = r'F:\\master\\code\\Lung_Nodule\\dataset\\LDCT_test_dataset\\CHESTCT_Test0014\\npy\\lobe_info.txt'\n",
    "\n",
    "filename = [r'F:\\master\\code\\Lung_Nodule\\dataset\\LDCT_test_dataset\\CHESTCT_Test0014', 'CHESTCT_Test0014']\n",
    "path, dir = filename\n",
    "img = np.load(os.path.join('%s\\\\npy\\\\%s.npy' % (path, dir)))\n",
    "img = img[np.newaxis,...] # (y, x, z) -> (1, y, x, z)\n",
    "## load lobe info\n",
    "with open(os.path.join('%s\\\\npy\\\\lobe_info.txt' %(path))) as f:\n",
    "    # ['z_start,z_end\\n',\n",
    "    #  'y_start,y_end,x_start,x_end']\n",
    "    lobe_info = f.readlines()[-2:]\n",
    "    lobes = crop_with_lobe(*lobe_info)\n",
    "\n",
    "with open(test_dicom_info, 'r') as f:\n",
    "    dicom_info = json.load(f)\n",
    "    nodules = dicom_info[\"bboxes\"]\n",
    "bboxes = []\n",
    "for nodule in nodules:\n",
    "    nodule = np.array(nodule)\n",
    "    cx, cy, cz = (nodule[1,:] + nodule[0,:]+1) /2\n",
    "    w,  h,  d  = (nodule[1,:] - nodule[0,:]+1)\n",
    "    bboxes.append(np.array([0.,cz, cy, cx, d, h, w]))\n",
    "bboxes = np.array(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 288, 128, 400, 64, 448)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lobes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0. ,  71.5, 171. , 332.5,   5. ,   6. ,   7. ],\n",
       "       [  0. , 251.5, 407. , 335. ,   9. ,  10. ,  12. ],\n",
       "       [  0. , 141. , 185.5, 368.5,   6. ,   7. ,   5. ]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 214 364 overlapped with nodule\n",
      "81 248 223\n"
     ]
    }
   ],
   "source": [
    "from utils.pybox import *\n",
    "import torch\n",
    "while True:\n",
    "    overlap_nodule = False\n",
    "    rz = np.random.randint(lobes[0]+64, lobes[1]-64)\n",
    "    ry = np.random.randint(lobes[2]+64, lobes[3]-64)\n",
    "    rx = np.random.randint(lobes[4]+64, lobes[5]-64)\n",
    "    overlaps = torch_overlap(bboxes[:,-6:], np.array([rz, ry, rx, 128, 128, 128]))\n",
    "    for overlap in overlaps:\n",
    "        if overlap[0] > 0.00:\n",
    "            overlap_nodule = True\n",
    "            break\n",
    "    if not overlap_nodule:\n",
    "        break\n",
    "    else:\n",
    "        print(f'{rz} {ry} {rx} overlapped with nodule')\n",
    "print(rz, ry, rx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(img, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m400\u001b[39m)\n\u001b[0;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 7\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (1, y, x, z) -> (1, z, y, x)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m images \u001b[38;5;241m=\u001b[39m img \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1000\u001b[39m    \u001b[38;5;66;03m# 0 ~ max\u001b[39;00m\n\u001b[0;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m (images \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m700\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m700\u001b[39m \u001b[38;5;66;03m# -1 ~ 1\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "Ds, De, Hs, He, Ws, We = crop_with_lobe(*lobe_info)\n",
    "# crop the lobe\n",
    "img = img[np.newaxis, Hs:He, Ws:We, Ds:De]\n",
    "\n",
    "img = np.clip(img, -1000, 400)\n",
    "img = img.astype(np.float32)\n",
    "img = img.transpose(0, 3, 1, 2) # (1, y, x, z) -> (1, z, y, x)\n",
    "images = img + 1000    # 0 ~ max\n",
    "images = (images - 700) / 700 # -1 ~ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 308 317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rz = np.random.randint(lobes[0]+64, lobes[1]-64)\n",
    "ry = np.random.randint(lobes[2]+64, lobes[3]-64)\n",
    "rx = np.random.randint(lobes[4]+64, lobes[5]-64)\n",
    "print(rz, ry, rx)\n",
    "torch_overlap(bboxes[:,-6:], np.array([rx, ry, rz, 128, 128,128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([i for i in range(10)])\n",
    "p = np.array([0.1 for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.array([0 for i in range(10)])\n",
    "for i in range(1000):\n",
    "    idcs = np.random.choice(len(a), 2, replace=False)\n",
    "    count[idcs] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201, 217, 185, 203, 194, 188, 193, 223, 218, 178])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "count2 = np.array([0 for i in range(10)])\n",
    "for i in range(1000):\n",
    "    idcs = np.random.choice(len(a), 2, replace=False, p=p)\n",
    "    p[idcs] *= 0.5\n",
    "    p /= p.sum()\n",
    "    count2[idcs]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choice with weight p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([198, 201, 200, 199, 200, 200, 200, 201, 200, 201])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209.0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749 251\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "decay = 0.9\n",
    "w = np.array([0.25,0.25,0.25,0.25])\n",
    "p = 0.75\n",
    "pos = 0\n",
    "neg = 0\n",
    "for i in range(1000):\n",
    "    idx = np.random.choice(4,1,p=w)\n",
    "    if idx <= 2:\n",
    "        pos +=1\n",
    "    else:\n",
    "        neg +=1\n",
    "        # w *= decay\n",
    "    w[idx] *= decay\n",
    "    w /= w.sum()\n",
    "\n",
    "print(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2345679012345678"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049309694441802776"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.bbox_reader_neg_nodulebased import BboxReader_NegNB\n",
    "from single_config import config, datasets_info\n",
    "data_dir = r'F:\\master\\code\\Lung_Nodule\\dataset\\ME_dataset'\n",
    "set_name = r'F:\\master\\code\\Lung_Nodule\\FL_lung_nodule_datasplit_ME_LDCT\\client0_train.txt'\n",
    "augtype = config['augtype']\n",
    "dataset = BboxReader_NegNB(data_dir, set_name, augtype, config, mode='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset.collate import train_collate\n",
    "train_loader = DataLoader(dataset, batch_size=6, shuffle=True,\n",
    "                              num_workers=6, pin_memory=True, collate_fn=train_collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = iter(train_loader)\n",
    "data = next(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 128, 128, 128])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, box, label = data\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, (input, truth_box, truth_label) in enumerate(train_loader):\n",
    "    print(truth_label[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "if np.random.choice(4, 1, p=[0.25,0.25,0.25,0.25]) < 2:\n",
    "    print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.round(439.49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6196473551637279 \n",
      " 0.6351118760755053 \n",
      " 0.6272843178919424\n"
     ]
    }
   ],
   "source": [
    "tpr = 61.9647355163728\n",
    "tnr = 99.99930071969138\n",
    "tol_pos = 1191\n",
    "tol_neg = 60633768\n",
    "recall = tpr/100\n",
    "precision = (tpr*tol_pos) / (tpr*tol_pos + (100-tnr)*tol_neg)\n",
    "f1 = 2*recall*precision/(recall+precision)\n",
    "print(recall,'\\n',precision,'\\n',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluationScript.tools import csvTools\n",
    "anno_path = r'F:\\master\\code\\LSSANet-main\\MsaNet_R_results\\ME_LDCT\\AdamW0.0003_Bs6x4_OHEM10_bb0_trueOHEM\\res\\81\\FROC\\submission_rpn.csv'\n",
    "annotations = csvTools.readCSV(anno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_nodules = {}\n",
    "for anno in annotations[1:]:\n",
    "    sid, x, y, z, w, h, d, prob = anno\n",
    "    if sid in dicom_nodules.keys():\n",
    "        dicom_nodules[sid] = np.append(dicom_nodules[sid],np.array([[float(prob),float(x),float(y),float(z),float(w),float(h),float(d)]]),axis=0)\n",
    "    else:\n",
    "        dicom_nodules[sid] = np.array([[float(prob),float(x),float(y),float(z),float(w),float(h),float(d)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pybox import torch_nms\n",
    "import torch\n",
    "import pandas as pd\n",
    "res_anno = []\n",
    "col_names = ['series_id', 'x_center', 'y_center', 'z_center', 'w_mm', 'h_mm', 'd_mm', 'probability']\n",
    "rpn_submission_path = r'F:\\master\\code\\LSSANet-main\\MsaNet_R_results\\ME_LDCT\\AdamW0.0003_Bs6x4_OHEM10_bb0_trueOHEM\\res\\81\\FROC\\submission_rpn_nms.csv'\n",
    "for key in dicom_nodules.keys():\n",
    "    nodules = dicom_nodules[key] + [0,0,0,0,2,2,2]\n",
    "    _, keep = torch_nms(torch.from_numpy(nodules.astype(np.float32)), 0.1)\n",
    "    outs = dicom_nodules[key][keep]\n",
    "    if len(keep) == 1:\n",
    "        outs = [outs]\n",
    "    for out in outs:\n",
    "        res_anno.append([key, str(out[1]), str(out[2]), str(out[3]), str(out[4]), str(out[5]), str(out[6]), str(out[0])])\n",
    "\n",
    "df = pd.DataFrame(res_anno, columns=col_names)\n",
    "df.to_csv(rpn_submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15525517,  0.19141471, -0.21032706,  0.96144958,  1.59379141,\n",
       "         1.24662869],\n",
       "       [ 0.13331018, -0.26620126,  0.16050524,  0.82845624,  0.82099605,\n",
       "         0.77365738],\n",
       "       [-0.02921804, -0.0392191 ,  0.10936578,  0.99105797,  0.90804561,\n",
       "         1.33544733]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "offset_ratio = 0.3\n",
    "r = np.random.rand(3,6)\n",
    "r[:,:3] = r[:,:3]* (2*offset_ratio) - offset_ratio\n",
    "r[:,3:] = r[:,3:]* (4*offset_ratio) + (1-2*offset_ratio) \n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02037814, 0.29622117, 0.1026084 , 0.06735228, 0.46886298,\n",
       "        0.3733121 ],\n",
       "       [0.69613119, 0.74726365, 0.32587972, 0.17928147, 0.36727735,\n",
       "        0.74830827],\n",
       "       [0.41786589, 0.50988561, 0.17664829, 0.43305144, 0.36943331,\n",
       "        0.01211162]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext2factor(arr, factor = 8):\n",
    "    arr[:,-3:] = (arr[:,-3:]+(factor-1))//factor *factor\n",
    "    return arr\n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[  1.0000,   0.8276,  83.1051, 103.0656,  97.2462,   6.0293,   8.1079,  8.2207],\n",
    "            [  1.0000,   0.8276,  84.2130, 100.7422,  95.9456,   6.3478,   9.6913,   8.7561],\n",
    "            [  1.0000,   0.8276,  83.1589, 104.3448,  97.2556,   8.9599,  12.2215,   6.3697],\n",
    "            [  1.0000,   0.8276,  83.6657, 102.6938,  96.9911,   7.3578,   7.4695,  11.9133],\n",
    "            [  1.0000,   0.7686,  71.2205,  93.9699,  65.5282,   5.8052,   7.8003,   7.9395],\n",
    "            [  1.0000,   0.7686,  72.3069,  96.1436,  65.8831,   8.1346,   8.0926,  11.9795],\n",
    "            [  1.0000,   0.7686,  70.7343,  94.8019,  67.8421,   5.7376,   9.0122,   8.2947],\n",
    "            [  1.0000,   0.7686,  69.8497,  93.8442,  64.0835,   8.2857,  11.9549,  11.9151]])\n",
    "\n",
    "a = a[:,[0,2,3,4,5,6,7]].astype(np.int32)\n",
    "a = ext2factor(a, factor=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  83, 103,  97,   8,   8,   8],\n",
       "       [  1,  84, 100,  95,   8,  16,   8],\n",
       "       [  1,  83, 104,  97,   8,  16,   8],\n",
       "       [  1,  83, 102,  96,   8,   8,  16],\n",
       "       [  1,  71,  93,  65,   8,   8,   8],\n",
       "       [  1,  72,  96,  65,   8,   8,  16],\n",
       "       [  1,  70,  94,  67,   8,  16,   8],\n",
       "       [  1,  69,  93,  64,   8,  16,  16]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.58652611 -0.08603455  1.50398733 -0.36092358  0.56573136\n",
      "    0.9860972 ]\n",
      "  [ 0.85878529  1.92066709 -1.3503354  -0.34804173 -0.52167635\n",
      "    0.46349816]\n",
      "  [-0.55351909  0.47625656  1.33297321  0.27401318 -1.5684168\n",
      "   -1.07712571]\n",
      "  [-1.91666554  0.54255647 -1.09073866  0.07678744 -0.13973057\n",
      "    1.10318014]]\n",
      "\n",
      " [[-0.81955314  0.0975312  -0.78889382 -0.02940587 -1.22535726\n",
      "    0.46930628]\n",
      "  [ 0.34801056 -1.74838815  1.40443309 -0.8201392   0.67418694\n",
      "    0.44477293]\n",
      "  [ 0.76509467 -2.90221815  0.39322361  0.33112346 -0.17325017\n",
      "    0.29825359]\n",
      "  [-1.63673518  1.33871714  0.2017215   0.38491992 -0.25858403\n",
      "   -1.45034208]]\n",
      "\n",
      " [[-0.25509721 -0.99032769 -0.75205615  1.44849796 -1.05677967\n",
      "    0.04867981]\n",
      "  [ 0.68567483 -0.02200537 -1.46960665  0.1385924   0.26559375\n",
      "    0.94404135]\n",
      "  [ 1.63269586 -0.23848062 -1.13844966 -0.25708323 -1.21329134\n",
      "   -0.13339221]\n",
      "  [-0.06256229  0.374907    1.01982907  0.14659279 -0.78167734\n",
      "   -0.34733871]]] tensor([[[ 0.5865, -0.0860,  1.5040, -0.3609,  0.5657,  0.9861],\n",
      "         [ 0.8588,  1.9207, -1.3503, -0.3480, -0.5217,  0.4635],\n",
      "         [-0.5535,  0.4763,  1.3330,  0.2740, -1.5684, -1.0771],\n",
      "         [-1.9167,  0.5426, -1.0907,  0.0768, -0.1397,  1.1032]],\n",
      "\n",
      "        [[-0.8196,  0.0975, -0.7889, -0.0294, -1.2254,  0.4693],\n",
      "         [ 0.3480, -1.7484,  1.4044, -0.8201,  0.6742,  0.4448],\n",
      "         [ 0.7651, -2.9022,  0.3932,  0.3311, -0.1733,  0.2983],\n",
      "         [-1.6367,  1.3387,  0.2017,  0.3849, -0.2586, -1.4503]],\n",
      "\n",
      "        [[-0.2551, -0.9903, -0.7521,  1.4485, -1.0568,  0.0487],\n",
      "         [ 0.6857, -0.0220, -1.4696,  0.1386,  0.2656,  0.9440],\n",
      "         [ 1.6327, -0.2385, -1.1384, -0.2571, -1.2133, -0.1334],\n",
      "         [-0.0626,  0.3749,  1.0198,  0.1466, -0.7817, -0.3473]]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "arr = np.random.randn(3,4,6)\n",
    "tensor = torch.from_numpy(arr)\n",
    "print(arr, tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_delta_variance(deltas, threshold):\n",
    "    deltas = deltas.view(-1,4,6)\n",
    "    var = torch.var(deltas, dim=1, unbiased=False)\n",
    "    mean_var = torch.mean(var, dim=1)\n",
    "    print(var, var.shape, mean_var)\n",
    "    keeps = torch.where(mean_var<=threshold)\n",
    "    return deltas[keeps,0], keeps\n",
    "\n",
    "def clip_delta_variance_np(deltas, threshold):\n",
    "    deltas = deltas.reshape(-1,4,6)\n",
    "    var = np.var(deltas, axis=1)\n",
    "    mean_var = np.mean(var, axis=1)\n",
    "    print(var, var.shape, mean_var)\n",
    "    keeps = np.where(mean_var<=threshold)\n",
    "    return deltas[keeps,0], keeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1997, 0.5455, 1.7532, 0.0751, 0.5948, 0.7550],\n",
      "        [0.9015, 2.6746, 0.6059, 0.2317, 0.4528, 0.6491],\n",
      "        [0.5510, 0.2467, 0.9231, 0.4150, 0.3324, 0.2416]], dtype=torch.float64) torch.Size([3, 6]) tensor([0.8206, 0.9193, 0.4516], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2551, -0.9903, -0.7521,  1.4485, -1.0568,  0.0487]],\n",
       "        dtype=torch.float64),\n",
       " (tensor([2]),))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_delta_variance(tensor, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.19973099 0.54550349 1.75318262 0.07507698 0.5948383  0.75500972]\n",
      " [0.90150385 2.67459574 0.605946   0.23171086 0.45283382 0.64908915]\n",
      " [0.55103076 0.24671458 0.92305489 0.41496215 0.33243857 0.24162199]] (3, 6) [0.82055702 0.9192799  0.45163716]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[-0.25509721, -0.99032769, -0.75205615,  1.44849796,\n",
       "          -1.05677967,  0.04867981]]]),\n",
       " (array([2], dtype=int64),))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_delta_variance_np(arr, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1327, 0.0238]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =  [ 0.1327,  0.1902, -0.0953,  0.2576,  0.3372, -0.6992,  0.0238,  0.0277,-0.0214, -0.0285,  0.0082,  0.0133]\n",
    "a[0::6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_dataset = BboxReader_NegNB(data_dir, train_set_list, augtype, config, mode='train')\n",
    "labeled_train_loader = DataLoader(labeled_train_dataset, batch_size=4, shuffle=True,\n",
    "                            num_workers=4, pin_memory=True, collate_fn=train_collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def swap(arr):\n",
    "    while True:\n",
    "        axisorder = torch.randperm(3)\n",
    "        if not torch.all(axisorder == torch.tensor([0, 1, 2])):\n",
    "            break\n",
    "    axisorder = torch.cat([torch.tensor([0,1]),axisorder+2], dim=0)\n",
    "    arr = arr.permute(*axisorder)\n",
    "    unswap = [0, 0, 0, 0, 0]\n",
    "    for i in range(5):\n",
    "        unswap[axisorder[i]] = i\n",
    "    return arr, torch.tensor(unswap)\n",
    "\n",
    "def flip(arr):\n",
    "    flipid = np.where([0,0,np.random.randint(2),np.random.randint(2),np.random.randint(2)])[0]\n",
    "    if flipid.sum() == 0:\n",
    "        flipid = [np.random.randint(2,5)]\n",
    "        print(flipid)\n",
    "    \n",
    "    arr = torch.flip(arr, dims=tuple(flipid))\n",
    "\n",
    "    return arr, flipid\n",
    "\n",
    "\n",
    "def unswap(arr, unswap):\n",
    "    return arr.permute(*unswap)\n",
    "\n",
    "def unflip(arr, unflip):\n",
    "    return torch.flip(arr, dims=tuple(unflip)).contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0, 1],\n",
      "           [2, 3]],\n",
      "\n",
      "          [[4, 5],\n",
      "           [6, 7]]]]])\n"
     ]
    }
   ],
   "source": [
    "swap_augment = []\n",
    "flip_augment = []\n",
    "scale = 2\n",
    "a = torch.tensor([[[[[i*(scale**2)+j*scale+k for k in range(scale)] for j in range(scale)] for i in range(scale)]]])\n",
    "print(a)\n",
    "for i in range(5):\n",
    "    a, unswapax = swap(a)\n",
    "    a, unflipid = flip(a)\n",
    "    # print(a)\n",
    "    swap_augment.append(unswapax)\n",
    "    flip_augment.append(unflipid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_augment.reverse()\n",
    "swap_augment.reverse()\n",
    "for i in range(5):\n",
    "    a = unflip(a, flip_augment[i])\n",
    "    a = unswap(a, swap_augment[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1],\n",
       "        [2, 3]],\n",
       "\n",
       "       [[4, 5],\n",
       "        [6, 7]]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where([0,0,np.random.randint(2),np.random.randint(2),np.random.randint(2)])[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(np.where([0,0,np.random.randint(2),np.random.randint(2),np.random.randint(2)])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([np.random.randint(2,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FGD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

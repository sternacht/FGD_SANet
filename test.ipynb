{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchors = [[5,5,5],[10,10,10],[15,15,15],[20,20,20],[30,30,30],[5,5,2.5],[10,10,5],[15,15,7.5],[20,20,10],[30,30,15]]\n",
    "# anchors = [[5,5,5],[10,10,10],[15,15,15],[20,20,20],[30,30,30]]\n",
    "anchors = [[6,6,6],[12,12,12],[24,24,24],[9,9,6],[18,18,12],[36,36,24]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_rpn_windows(f_shape):\n",
    "    \"\"\"\n",
    "    Generating anchor boxes at each voxel on the feature map,\n",
    "    the center of the anchor box on each voxel corresponds to center\n",
    "    on the original input image.\n",
    "\n",
    "    return\n",
    "    windows: list of anchor boxes, [z, y, x, d, h, w]\n",
    "    \"\"\"\n",
    "    stride = 4\n",
    "    anchors = np.asarray(anchors)\n",
    "    offset = (float(stride) - 1) / 2\n",
    "    _, _, D, H, W = f_shape\n",
    "    oz = np.arange(offset, offset + stride * (D - 1) + 1, stride)\n",
    "    oh = np.arange(offset, offset + stride * (H - 1) + 1, stride)\n",
    "    ow = np.arange(offset, offset + stride * (W - 1) + 1, stride)\n",
    "\n",
    "    windows = []\n",
    "    for z, y, x, a in itertools.product(oz, oh , ow , anchors):\n",
    "        windows.append([z, y, x, a[0], a[1], a[2]])\n",
    "    windows = np.array(windows)\n",
    "\n",
    "    return windows\n",
    "\n",
    "def find_nearest(x,y,z):\n",
    "    stride = 4\n",
    "    start = (stride-1) / 2\n",
    "    z_stride = 4\n",
    "    z_start = (z_stride-1) / 2\n",
    "    xl = ((x - start) // stride) *stride + start\n",
    "    xr = xl+stride\n",
    "    xn = xl if x-xl < xr-x else xr\n",
    "\n",
    "    yl = ((y - start) // stride) *stride + start\n",
    "    yr = yl+stride\n",
    "    yn = yl if y-yl < yr-y else yr\n",
    "\n",
    "    zl = ((z - z_start) // z_stride) *z_stride + z_start\n",
    "    zr = zl+z_stride\n",
    "    zn = zl if z-zl < zr-z else zr\n",
    "\n",
    "    return xn, yn, zn\n",
    "\n",
    "def IoU(cord1, shape, cord2, shape2):\n",
    "    x1, y1, z1 = cord1\n",
    "    w, h, d = shape      # nodule bbox shape\n",
    "    x2, y2, z2 = cord2\n",
    "    aw, ah, ad = shape2  # anchor shape\n",
    "\n",
    "    x_overlap = min(x1+w/2, x2+aw/2) - max(x1-w/2, x2-aw/2) if (max(x1+w/2, x2+aw/2)-min(x1-w/2, x2-aw/2)) < w+aw else 0\n",
    "    y_overlap = min(y1+h/2, y2+ah/2) - max(y1-h/2, y2-ah/2) if (max(y1+h/2, y2+ah/2)-min(y1-h/2, y2-ah/2)) < h+ah else 0\n",
    "    z_overlap = min(z1+d/2, z2+ad/2) - max(z1-d/2, z2-ad/2) if (max(z1+d/2, z2+ad/2)-min(z1-d/2, z2-ad/2)) < d+ad else 0\n",
    "    v = x_overlap * y_overlap*z_overlap\n",
    "    return v / (w*h*d+aw*ah*ad-v)\n",
    "\n",
    "def distance(cord1, cord2):\n",
    "    x1, y1, z1 = cord1\n",
    "    x2, y2, z2 = cord2\n",
    "    return ((x1-x2)**2 + (y1-y2)**2 + (z1-z2)**2)\n",
    "\n",
    "def DIoU(cord1, shape, cord2, shape2):\n",
    "    x1, y1, z1 = cord1\n",
    "    w, h, d = shape      # nodule bbox shape\n",
    "    x2, y2, z2 = cord2\n",
    "    aw, ah, ad = shape2  # anchor shape\n",
    "\n",
    "    c1 = [max(x1+w/2, x2+aw/2), max(y1+h/2, y2+ah/2), max(z1+d/2, z2+ad/2)]\n",
    "    c2 = [min(x1-w/2, x2-aw/2), min(y1-h/2, y2-ah/2), min(z1-d/2, z2-ad/2)]\n",
    "\n",
    "    return IoU(cord1, shape, cord2, shape2) - distance(cord1, cord2)/distance(c1, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[329, 1491, 76, 1387, 615, 36] \n",
      " [0, 3, 46, 1761] \n",
      " [1, 1809]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "nodule_distribution = [0,0,0,0]\n",
    "anchor_distribution = [0 for i in range(len(anchors))]\n",
    "\n",
    "train_txt = r'F:\\master\\code\\Lung_Nodule\\FL_lung_nodule_datasplit_ME_LDCT\\client0_train.txt'\n",
    "with open(train_txt, 'r') as f:\n",
    "    train_files = f.readlines()\n",
    "anchor_count = [0,0]\n",
    "bad_nodule = []\n",
    "for train_file in train_files[1:]:\n",
    "    p1, p2 = train_file.replace('\\n','').split(',')\n",
    "\n",
    "    dicom_nodule_path = os.path.join(p1, 'mask', f'{p2}_nodule_count.json') # CHEST1001_nodule_count\n",
    "    with open(dicom_nodule_path, 'r') as f:\n",
    "        dicom_nodule = json.load(f)\n",
    "    nodules = dicom_nodule['bboxes']\n",
    "    good_nodule = 0\n",
    "    for nodule in nodules:\n",
    "        cx = (nodule[0][0]+nodule[1][0])/2\n",
    "        cy = (nodule[0][1]+nodule[1][1])/2\n",
    "        cz = (nodule[0][2]+nodule[1][2])/2\n",
    "        w = (nodule[1][0]-nodule[0][0])+4\n",
    "        h = (nodule[1][1]-nodule[0][1])+4\n",
    "        d = (nodule[1][2]-nodule[0][2])+4\n",
    "\n",
    "        size = (4/3)*3.1415*(h*w*d / 6)\n",
    "        if size <= 52:\n",
    "            size_idx = 0\n",
    "        elif size <= 113:\n",
    "            size_idx = 1\n",
    "        elif size <= 268:\n",
    "            size_idx = 2\n",
    "        else:\n",
    "            size_idx = 3\n",
    "\n",
    "        nodule_distribution[size_idx] += 1\n",
    "\n",
    "        nearest_point = find_nearest(cx, cy, cz)\n",
    "\n",
    "        for i, anchor in enumerate(anchors):\n",
    "            if IoU([cx,cy,cz], [w,h,d], nearest_point, anchor) >= 0.3:\n",
    "                anchor_distribution[i] += 1\n",
    "                good_nodule = 1\n",
    "\n",
    "        anchor_count[good_nodule] += 1\n",
    "        if not good_nodule:\n",
    "            bad_nodule.append([size_idx, w,h,d])\n",
    "print(anchor_distribution, '\\n', nodule_distribution, '\\n', anchor_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19259244358604574"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '217.5776\t324.21124\t182.56894\t7.231955\t7.961705\t5.010294'\n",
    "b = '217.84135\t317.79977\t182.8455\t8.86131\t8.294744\t5.4044294'\n",
    "a = a.split('\\t')\n",
    "a_cord = [float(a_) for a_ in a[:3]]\n",
    "a_shape = [float(a_)+2. for a_ in a[3:]]\n",
    "b = b.split('\\t')\n",
    "b_cord = [float(b_) for b_ in b[:3]]\n",
    "b_shape = [float(b_)+2. for b_ in b[3:]]\n",
    "IoU(a_cord, a_shape, b_cord, b_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([9.231955, 9.961705, 7.010294], [10.86131, 10.294744, 7.4044294])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_shape, b_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_path = r'F:\\master\\code\\LSSANet-main\\MsaNet_R_results\\ME_LDCT\\AdamW0.0003_Bs6x4_OHEM10_bb0\\res\\72\\FROC\\submission_rpn.csv'\n",
    "gt_path  = r'F:\\master\\code\\Lung_Nodule\\FL_lung_nodule_datasplit_ME_LDCT\\client0_test_annotation.csv'\n",
    "\n",
    "import csv\n",
    "sub_by_file = {}\n",
    "with open(sub_path, newline='') as fs:\n",
    "    subs = csv.reader(fs,)\n",
    "    for sub in subs:\n",
    "        if sub[0] in sub_by_file.keys():\n",
    "            sub_by_file[sub[0]].append([float(xyzhwd) for xyzhwd in sub[1:8]])\n",
    "        elif sub[0] != 'series_id':\n",
    "            sub_by_file[sub[0]] = [[float(xyzhwd) for xyzhwd in sub[1:8]]]\n",
    "\n",
    "gt_by_file  = {}\n",
    "with open(gt_path, newline='') as fg:\n",
    "    gts = csv.reader(fg)\n",
    "    for gt in gts:\n",
    "        if gt[0] in gt_by_file.keys():\n",
    "            gt_by_file[gt[0]].append([float(xyzhwd) for xyzhwd in gt[1:8]])\n",
    "        elif gt[0] != 'series_id':\n",
    "            gt_by_file[gt[0]] = [[float(xyzhwd) for xyzhwd in gt[1:8]]]\n",
    "\n",
    "files = list(gt_by_file.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = []\n",
    "FN = []\n",
    "gt_IoU = []\n",
    "gt_DIoU = []\n",
    "for file in files:\n",
    "    gts = gt_by_file[file]\n",
    "    subs = sub_by_file[file]\n",
    "    for gt in gts:\n",
    "        is_cand = 0\n",
    "        re_detected = 0\n",
    "        max_iou = 0.0\n",
    "        max_diou = 0.0\n",
    "        gt_cord, gt_shape = gt[:3], gt[3:6]\n",
    "        for sub in subs:\n",
    "            iou = IoU(gt_cord, gt_shape, sub[:3], sub[3:6])\n",
    "            if iou >= 0.1:\n",
    "                if is_cand:\n",
    "                    re_detected += 1\n",
    "                is_cand = 1\n",
    "            max_iou = max(max_iou, iou)\n",
    "            max_diou = max(max_diou, DIoU(gt_cord, gt_shape, sub[:3], sub[3:6]))\n",
    "        gt_IoU.append([max_iou, max_diou])\n",
    "        gt_DIoU.append(max_diou)\n",
    "        if is_cand:\n",
    "            TP.append(max_iou)\n",
    "        else:\n",
    "            FN.append(max_iou)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "from single_config import config\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def crop_with_lobe(z:str, yx:str):\n",
    "    zlobe = z.replace('\\n','').split(',')\n",
    "    Ds, De = [int(z_) for z_ in zlobe]\n",
    "    yxlobe = yx.replace('\\n','').split(',')\n",
    "    Hs, He, Ws, We = [int(yx_) for yx_ in yxlobe]\n",
    "    \n",
    "    # align to 16\n",
    "    align = 16\n",
    "    Ds = (Ds//align)*align\n",
    "    De = ((De+align-1)//align)*align\n",
    "    Hs = (Hs//align)*align\n",
    "    He = ((He+align-1)//align)*align\n",
    "    Ws = (Ws//align)*align\n",
    "    We = ((We+align-1)//align)*align\n",
    "\n",
    "    return (Ds, De, Hs, He, Ws, We)\n",
    "\n",
    "def load_img(filename):\n",
    "    path, dir = filename.split(',')\n",
    "    img = np.load('%s\\\\npy\\\\%s.npy' % (path, dir))\n",
    "    # img = img[np.newaxis,...] # (y, x, z) -> (1, y, x, z)\n",
    "    ## load lobe info\n",
    "    with open('%s\\\\npy\\\\lobe_info.txt' %(path)) as f:\n",
    "        lobe_info = f.readlines()[-2:]\n",
    "    with open('%s\\\\mask\\\\%s_nodule_count.json' % (path, dir)) as f:\n",
    "        nodule_count = json.load(f)\n",
    "    bboxes = nodule_count['bboxes']\n",
    "    Ds, De, Hs, He, Ws, We = crop_with_lobe(*lobe_info)\n",
    "    # crop the lobe\n",
    "    img = img[np.newaxis, Hs:He, Ws:We, Ds:De]\n",
    "\n",
    "    img = np.clip(img, -1000, 400)\n",
    "    img = img.astype(np.float32)\n",
    "    img = img.transpose(0, 3, 1, 2) # (1, y, x, z) -> (1, z, y, x)\n",
    "    images = img + 1000    # 0 ~ max\n",
    "    images = (images - 700) / 700 # -1 ~ 1\n",
    "    return images, (Ds, De, Hs, He, Ws, We), bboxes\n",
    "\n",
    "class Crop(object):\n",
    "    def __init__(self, config):\n",
    "        self.crop_size = config['crop_size']\n",
    "        self.bound_size = config['bound_size']\n",
    "        self.stride = config['stride']\n",
    "        self.pad_value = config['pad_value']\n",
    "\n",
    "    def __call__(self, imgs, target, bboxes, lobe=None, isScale=False, isRand=False):\n",
    "        '''\n",
    "        img: 3D image loading from npy, (1, d, h, w)\n",
    "        target: one nodule\n",
    "        bboxes: all nodules in series\n",
    "        '''\n",
    "        if isScale:\n",
    "            radiusLim = [8.,120.]\n",
    "            scaleLim = [0.75,1.25]\n",
    "            scaleRange = [np.min([np.max([(radiusLim[0]/target[3]),scaleLim[0]]),1])\n",
    "                         ,np.max([np.min([(radiusLim[1]/target[3]),scaleLim[1]]),1])]\n",
    "            scale = np.random.rand()*(scaleRange[1]-scaleRange[0])+scaleRange[0]\n",
    "            crop_size = (np.array(self.crop_size).astype('float')/scale).astype('int')\n",
    "        else:\n",
    "            crop_size = self.crop_size\n",
    "        bound_size = self.bound_size\n",
    "        target = np.copy(target)\n",
    "        bboxes = np.copy(bboxes)\n",
    "        start = []\n",
    "        for i in range(3):\n",
    "            # start.append(int(target[i] - crop_size[i] / 2))\n",
    "            if not isRand:\n",
    "                # crop the sample base on target\n",
    "                r = target[i+3]/2\n",
    "                s = np.floor(target[i] - r) + 1 - bound_size\n",
    "                e = np.ceil(target[i] + r) + 1 + bound_size - crop_size[i]\n",
    "            else:\n",
    "                # crop the sample randomly\n",
    "                s = np.max([imgs.shape[i+1]-crop_size[i]/2, imgs.shape[i+1]/2+bound_size])\n",
    "                e = np.min([crop_size[i]/2, imgs.shape[i+1]/2-bound_size])\n",
    "                target = np.array([np.nan, np.nan, np.nan, np.nan])\n",
    "            if s > e:\n",
    "                i_start = np.random.randint(e, s)\n",
    "                i_start = max(min(i_start, imgs.shape[i+1]-crop_size[i]),0)\n",
    "                start.append(i_start)#!\n",
    "            else:\n",
    "                start.append(int(target[i])-crop_size[i]/2+np.random.randint(-bound_size/2,bound_size/2))\n",
    "\n",
    "        coord=[]\n",
    "        pad = []\n",
    "        pad.append([0,0])\n",
    "\n",
    "        for i in range(3):\n",
    "            leftpad = max(0,-start[i]) # how many pixel need to pad on the left side\n",
    "            rightpad = max(0,start[i]+crop_size[i]-imgs.shape[i+1]) # how many pixel need to pad on the right side\n",
    "            pad.append([leftpad,rightpad])\n",
    "            \n",
    "        crop = imgs[:,\n",
    "            int(max(start[0],0)):int(min(start[0] + int(crop_size[0]),imgs.shape[1])),\n",
    "            int(max(start[1],0)):int(min(start[1] + int(crop_size[1]),imgs.shape[2])),\n",
    "            int(max(start[2],0)):int(min(start[2] + int(crop_size[2]),imgs.shape[3]))]\n",
    "\n",
    "        crop = np.pad(crop, pad, 'constant', constant_values=0.67142)\n",
    "        for i in range(3):\n",
    "            target[i] = target[i] - start[i]\n",
    "        for i in range(len(bboxes)):\n",
    "            for j in range(3):\n",
    "                bboxes[i][j] = bboxes[i][j] - start[j]\n",
    "\n",
    "        if isScale:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                crop = zoom(crop, [1, scale, scale, scale], order=1)\n",
    "            newpad = self.crop_size[0] - crop.shape[1:][0]\n",
    "            if newpad < 0:\n",
    "                crop = crop[:, :-newpad, :-newpad, :-newpad]\n",
    "            elif newpad > 0:\n",
    "                pad2 = [[0, 0], [0, newpad], [0, newpad], [0, newpad]]\n",
    "                crop = np.pad(crop, pad2, 'constant', constant_values=0.67142)\n",
    "            for i in range(6):\n",
    "                target[i] = target[i] * scale\n",
    "            for i in range(len(bboxes)):\n",
    "                for j in range(6):\n",
    "                    bboxes[i][j] = bboxes[i][j] * scale\n",
    "\n",
    "        return crop, target, bboxes, coord\n",
    "\n",
    "crop = Crop(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = r'F:\\master\\code\\Lung_Nodule\\FL_lung_nodule_datasplit_ME_LDCT\\client0_test.txt'\n",
    "with open(test_txt, 'r') as f:\n",
    "    files = f.readlines()\n",
    "imgs, lobe_info, bboxes = load_img(files[1])\n",
    "bboxes = bboxes - [lobe_info[0], lobe_info[2], lobe_info[4], 0, 0, 0]\n",
    "for bbox in bboxes:\n",
    "    sample, target, bboxes, coord = crop(imgs, bbox, bboxes, isScale=False, isRand=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from utils.util import crop_with_lobe\n",
    "test_dicom_npy = r'F:\\master\\code\\Lung_Nodule\\dataset\\LDCT_test_dataset\\CHESTCT_Test0014\\npy\\CHESTCT_Test0014.npy'\n",
    "test_dicom_info = r'F:\\master\\code\\Lung_Nodule\\dataset\\LDCT_test_dataset\\CHESTCT_Test0014\\mask\\CHESTCT_Test0014_nodule_count.json'\n",
    "test_dicom_lobe = r'F:\\master\\code\\Lung_Nodule\\dataset\\LDCT_test_dataset\\CHESTCT_Test0014\\npy\\lobe_info.txt'\n",
    "\n",
    "filename = [r'F:\\master\\code\\Lung_Nodule\\dataset\\LDCT_test_dataset\\CHESTCT_Test0014', 'CHESTCT_Test0014']\n",
    "path, dir = filename\n",
    "img = np.load(os.path.join('%s\\\\npy\\\\%s.npy' % (path, dir)))\n",
    "img = img[np.newaxis,...] # (y, x, z) -> (1, y, x, z)\n",
    "## load lobe info\n",
    "with open(os.path.join('%s\\\\npy\\\\lobe_info.txt' %(path))) as f:\n",
    "    # ['z_start,z_end\\n',\n",
    "    #  'y_start,y_end,x_start,x_end']\n",
    "    lobe_info = f.readlines()[-2:]\n",
    "    lobes = crop_with_lobe(*lobe_info)\n",
    "\n",
    "with open(test_dicom_info, 'r') as f:\n",
    "    dicom_info = json.load(f)\n",
    "    nodules = dicom_info[\"bboxes\"]\n",
    "bboxes = []\n",
    "for nodule in nodules:\n",
    "    nodule = np.array(nodule)\n",
    "    cx, cy, cz = (nodule[1,:] + nodule[0,:]+1) /2\n",
    "    w,  h,  d  = (nodule[1,:] - nodule[0,:]+1)\n",
    "    bboxes.append(np.array([0.,cz, cy, cx, d, h, w]))\n",
    "bboxes = np.array(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 288, 128, 400, 64, 448)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lobes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0. ,  71.5, 171. , 332.5,   5. ,   6. ,   7. ],\n",
       "       [  0. , 251.5, 407. , 335. ,   9. ,  10. ,  12. ],\n",
       "       [  0. , 141. , 185.5, 368.5,   6. ,   7. ,   5. ]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 214 364 overlapped with nodule\n",
      "81 248 223\n"
     ]
    }
   ],
   "source": [
    "from utils.pybox import *\n",
    "import torch\n",
    "while True:\n",
    "    overlap_nodule = False\n",
    "    rz = np.random.randint(lobes[0]+64, lobes[1]-64)\n",
    "    ry = np.random.randint(lobes[2]+64, lobes[3]-64)\n",
    "    rx = np.random.randint(lobes[4]+64, lobes[5]-64)\n",
    "    overlaps = torch_overlap(bboxes[:,-6:], np.array([rz, ry, rx, 128, 128, 128]))\n",
    "    for overlap in overlaps:\n",
    "        if overlap[0] > 0.00:\n",
    "            overlap_nodule = True\n",
    "            break\n",
    "    if not overlap_nodule:\n",
    "        break\n",
    "    else:\n",
    "        print(f'{rz} {ry} {rx} overlapped with nodule')\n",
    "print(rz, ry, rx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(img, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m400\u001b[39m)\n\u001b[0;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 7\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (1, y, x, z) -> (1, z, y, x)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m images \u001b[38;5;241m=\u001b[39m img \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1000\u001b[39m    \u001b[38;5;66;03m# 0 ~ max\u001b[39;00m\n\u001b[0;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m (images \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m700\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m700\u001b[39m \u001b[38;5;66;03m# -1 ~ 1\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "Ds, De, Hs, He, Ws, We = crop_with_lobe(*lobe_info)\n",
    "# crop the lobe\n",
    "img = img[np.newaxis, Hs:He, Ws:We, Ds:De]\n",
    "\n",
    "img = np.clip(img, -1000, 400)\n",
    "img = img.astype(np.float32)\n",
    "img = img.transpose(0, 3, 1, 2) # (1, y, x, z) -> (1, z, y, x)\n",
    "images = img + 1000    # 0 ~ max\n",
    "images = (images - 700) / 700 # -1 ~ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 308 317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rz = np.random.randint(lobes[0]+64, lobes[1]-64)\n",
    "ry = np.random.randint(lobes[2]+64, lobes[3]-64)\n",
    "rx = np.random.randint(lobes[4]+64, lobes[5]-64)\n",
    "print(rz, ry, rx)\n",
    "torch_overlap(bboxes[:,-6:], np.array([rx, ry, rz, 128, 128,128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([i for i in range(10)])\n",
    "p = np.array([0.1 for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.array([0 for i in range(10)])\n",
    "for i in range(1000):\n",
    "    idcs = np.random.choice(len(a), 2, replace=False)\n",
    "    count[idcs] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201, 217, 185, 203, 194, 188, 193, 223, 218, 178])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "count2 = np.array([0 for i in range(10)])\n",
    "for i in range(1000):\n",
    "    idcs = np.random.choice(len(a), 2, replace=False, p=p)\n",
    "    p[idcs] *= 0.5\n",
    "    p /= p.sum()\n",
    "    count2[idcs]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choice with weight p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([198, 201, 200, 199, 200, 200, 200, 201, 200, 201])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209.0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749 251\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "decay = 0.9\n",
    "w = np.array([0.25,0.25,0.25,0.25])\n",
    "p = 0.75\n",
    "pos = 0\n",
    "neg = 0\n",
    "for i in range(1000):\n",
    "    idx = np.random.choice(4,1,p=w)\n",
    "    if idx <= 2:\n",
    "        pos +=1\n",
    "    else:\n",
    "        neg +=1\n",
    "        # w *= decay\n",
    "    w[idx] *= decay\n",
    "    w /= w.sum()\n",
    "\n",
    "print(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2345679012345678"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049309694441802776"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.bbox_reader_neg_nodulebased import BboxReader_NegNB\n",
    "from single_config import config, datasets_info\n",
    "data_dir = r'F:\\master\\code\\Lung_Nodule\\dataset\\ME_dataset'\n",
    "set_name = r'F:\\master\\code\\Lung_Nodule\\FL_lung_nodule_datasplit_ME_LDCT\\client0_train.txt'\n",
    "augtype = config['augtype']\n",
    "dataset = BboxReader_NegNB(data_dir, set_name, augtype, config, mode='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset.collate import train_collate\n",
    "train_loader = DataLoader(dataset, batch_size=6, shuffle=True,\n",
    "                              num_workers=6, pin_memory=True, collate_fn=train_collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, (input, truth_box, truth_label) in enumerate(train_loader):\n",
    "    print(truth_label[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "if np.random.choice(4, 1, p=[0.25,0.25,0.25,0.25]) < 2:\n",
    "    print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.round(439.49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6196473551637279 \n",
      " 0.6351118760755053 \n",
      " 0.6272843178919424\n"
     ]
    }
   ],
   "source": [
    "tpr = 61.9647355163728\n",
    "tnr = 99.99930071969138\n",
    "tol_pos = 1191\n",
    "tol_neg = 60633768\n",
    "recall = tpr/100\n",
    "precision = (tpr*tol_pos) / (tpr*tol_pos + (100-tnr)*tol_neg)\n",
    "f1 = 2*recall*precision/(recall+precision)\n",
    "print(recall,'\\n',precision,'\\n',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluationScript.tools import csvTools\n",
    "anno_path = r'F:\\master\\code\\LSSANet-main\\MsaNet_R_results\\ME_LDCT\\AdamW0.0003_Bs6x4_fOHEM10_bb0_nbneg\\res\\72\\FROC\\submission_rpn.csv'\n",
    "annotations = csvTools.readCSV(anno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_nodules = {}\n",
    "for anno in annotations[1:]:\n",
    "    sid, x, y, z, w, h, d, prob = anno\n",
    "    if sid in dicom_nodules.keys():\n",
    "        dicom_nodules[sid] = np.append(dicom_nodules[sid],np.array([[float(x),float(y),float(z),float(w),float(h),float(d), float(prob)]]),axis=0)\n",
    "    else:\n",
    "        dicom_nodules[sid] = np.array([[float(x),float(y),float(z),float(w),float(h),float(d), float(prob)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , 11.679075  , 10.78574   ,\n",
       "         7.5788956 ,  0.9593786 ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  5.172542  ,  4.8349504 ,\n",
       "         3.7945857 ,  0.65252703],\n",
       "       [ 0.        ,  0.        ,  0.        , 12.183861  , 14.153741  ,\n",
       "         7.0153294 ,  0.63018936],\n",
       "       [ 0.        ,  0.        ,  0.        ,  5.922808  ,  5.99413   ,\n",
       "         4.313506  ,  0.5809836 ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  5.778343  ,  5.52759   ,\n",
       "         4.060829  ,  0.5738536 ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  6.453224  ,  6.188073  ,\n",
       "         4.099965  ,  0.5635797 ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  6.79552   ,  6.665095  ,\n",
       "         4.5868487 ,  0.5495065 ]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicom_nodules['F:\\\\master\\\\code\\\\Lung_Nodule\\\\dataset\\\\ME_dataset\\\\CHEST1102\\\\mask\\\\CHEST1102_nodule_count.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pybox import torch_nms\n",
    "import torch\n",
    "test = dicom_nodules['F:\\master\\code\\Lung_Nodule\\dataset\\ME_dataset\\CHESTCT1747\\mask\\CHESTCT1747_nodule_count.json'] + [0,0,0,2,2,2,0]\n",
    "out,_ = torch_nms(torch.from_numpy(test.astype(np.float32)), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000, 10.8613, 10.2947,  7.4044,  0.9506],\n",
       "        [ 0.0000,  0.0000,  0.0000,  9.7671, 10.4456,  7.5393,  0.9115],\n",
       "        [ 0.0000,  0.0000,  0.0000, 13.1707, 13.3566,  9.7485,  0.8455],\n",
       "        [ 0.0000,  0.0000,  0.0000,  7.9193,  7.4513,  6.1191,  0.6550],\n",
       "        [ 0.0000,  0.0000,  0.0000,  9.0956,  8.8516,  6.4882,  0.5422]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[217.8414, 317.7998, 182.8455,  10.8613,  10.2947,   7.4044,   0.9506],\n",
       "        [217.8274, 317.9329, 187.8406,   9.7671,  10.4456,   7.5393,   0.9115],\n",
       "        [153.9819, 306.9145,  79.3042,  13.1707,  13.3566,   9.7485,   0.8455],\n",
       "        [206.7219, 178.6786, 178.4819,  13.7577,  14.5464,   9.9385,   0.6905],\n",
       "        [162.8256, 177.9178, 219.1056,   7.9193,   7.4513,   6.1191,   0.6550],\n",
       "        [217.5776, 324.2112, 182.5689,   9.2320,   9.9617,   7.0103,   0.6323],\n",
       "        [182.0679, 310.7646,  67.2307,   9.9825,   9.6879,   7.0060,   0.5791],\n",
       "        [182.0941, 310.5662,  71.1550,   9.0956,   8.8516,   6.4882,   0.5422],\n",
       "        [162.8644, 177.9783, 215.4896,   7.6482,   7.4470,   6.0287,   0.5159]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FGD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
